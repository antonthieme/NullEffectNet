{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import ast\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, precision_recall_curve\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "sys.path.append('../src/null-effect-net')\n",
    "import utils\n",
    "import models\n",
    "import dataset\n",
    "import train_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLPAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=1024, latent_dim=256, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            # No activation here; assume reconstruction loss (e.g., MSE) will be used\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, latent):\n",
    "        return self.decoder(latent)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encode(x)\n",
    "        reconstructed = self.decode(latent)\n",
    "        return reconstructed\n",
    "\n",
    "    def compute_loss(self, reconstructed, original):\n",
    "        return F.mse_loss(reconstructed, original)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New simple dataset for autoencoder\n",
    "class EmbeddingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, node_features_df, device):\n",
    "        self.embeddings = torch.tensor(np.stack(node_features_df['Concat Embedding'].values), dtype=torch.float32, device=device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        reconstructed = model(batch)\n",
    "        loss = model.compute_loss(reconstructed, batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    return {'loss': avg_loss}\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            reconstructed = model(batch)\n",
    "            loss = model.compute_loss(reconstructed, batch)\n",
    "\n",
    "            total_loss += loss.item() * batch.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    return {'loss': avg_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_utils.set_seed(42)\n",
    "\n",
    "with open('../data/embeddings.pkl', 'rb') as f:\n",
    "    node_features_df = pickle.load(f)\n",
    "\n",
    "node_features_df['Concat Embedding'] = node_features_df['ESM Embedding'] + node_features_df['SubCell Embedding'] + node_features_df['PINNACLE Embedding']\n",
    "\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "\n",
    "active_nodes_df = pd.read_csv('../data/expression_reference/expression_reference.csv', index_col=0)\n",
    "\n",
    "train_dataset = EmbeddingDataset(node_features_df.iloc[:int(0.9*len(node_features_df))], device=device)\n",
    "val_dataset = EmbeddingDataset(node_features_df.iloc[int(0.9*len(node_features_df)):], device=device)\n",
    "\n",
    "input_dim = len(node_features_df['Concat Embedding'][0])\n",
    "\n",
    "model = SimpleMLPAutoencoder(input_dim, hidden_dim=1024, latent_dim=256, dropout_rate=0.3).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Epoch 1/20 ====\n",
      "Train loss: 0.0006\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 2/20 ====\n",
      "Train loss: 0.0006\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 3/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 4/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 5/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 6/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 7/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 8/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 9/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 10/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 11/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 12/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 13/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 14/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 15/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 16/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 17/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 18/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 19/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n",
      "\n",
      "==== Epoch 20/20 ====\n",
      "Train loss: 0.0005\n",
      "Val loss: 0.0005\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n==== Epoch {epoch+1}/{num_epochs} ====\")\n",
    "    train_metrics = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    val_metrics = evaluate(model, val_loader, device)\n",
    "\n",
    "    print(f\"Train loss: {train_metrics['loss']:.4f}\")\n",
    "    print(f\"Val loss: {val_metrics['loss']:.4f}\")\n",
    "\n",
    "    if val_metrics['loss'] < best_val_loss:\n",
    "        best_val_loss = val_metrics['loss']\n",
    "        train_utils.save_model(model, \"../models/simple_mlp_autoencoder_best.pt\")\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
