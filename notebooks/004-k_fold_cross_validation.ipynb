{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import sys\n",
    "import pandas as pd\n",
    "import ast\n",
    "import pickle\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, precision_recall_curve\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "sys.path.append('../src/null-effect-net')\n",
    "import utils\n",
    "import models\n",
    "import dataset\n",
    "import train_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_utils.set_seed(42)\n",
    "\n",
    "id_map_df = pd.read_csv('../data/id_mappings/gene_ref.tsv', sep='\\t')\n",
    "\n",
    "with open('../data/embeddings.pkl', 'rb') as f:\n",
    "    node_features_df = pickle.load(f)\n",
    "\n",
    "node_features_df['Concat Embedding'] = node_features_df['PINNACLE Embedding'] + node_features_df['SubCell Embedding'] + node_features_df['ESM Embedding']\n",
    "\n",
    "train_df = pd.read_csv('../data/perturbation_screens/e_distance/train.csv', index_col=0)\n",
    "\n",
    "active_nodes_df = pd.read_csv('../data/expression_reference/expression_reference.csv', index_col=0)\n",
    "\n",
    "G = nx.read_edgelist('../data/networks/global_ppi_edgelist.txt')\n",
    "\n",
    "ensembl_to_node = dict(zip(id_map_df['Ensembl gene ID'], id_map_df['Approved symbol']))\n",
    "node_to_ensembl = dict(zip(id_map_df['Approved symbol'], id_map_df['Ensembl gene ID']))\n",
    "\n",
    "G = nx.relabel_nodes(G, node_to_ensembl)\n",
    "\n",
    "# Get set of nodes that have features\n",
    "valid_nodes = set(node_features_df['Ensembl ID'])\n",
    "\n",
    "# Remove nodes from G that are not in valid_nodes\n",
    "G.remove_nodes_from([n for n in list(G.nodes) if n not in valid_nodes])\n",
    "\n",
    "node_to_idx = {node: idx for idx, node in enumerate(G.nodes())}\n",
    "\n",
    "train_df = train_df[train_df['Target'].isin(G.nodes())]\n",
    "\n",
    "# Convert to edge_index format\n",
    "data = from_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation setup\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Full train_df (already filtered to nodes in G) is used\n",
    "full_train_df = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features_df['Concatenated Embedding'] = node_features_df['PINNACLE Embedding'] + node_features_df['SubCell Embedding'] + node_features_df['ESM Embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding_to_use in ['PINNACLE Embedding', 'ESM Embedding', 'SubCell Embedding', 'Concatenated Embedding']:\n",
    "\n",
    "    node_features_df['Concat Embedding'] = node_features_df[embedding_to_use]\n",
    "\n",
    "    features = torch.stack([\n",
    "        torch.tensor(node_features_df.set_index('Ensembl ID').loc[idx]['Concat Embedding'])\n",
    "        for idx in G.nodes()\n",
    "    ])\n",
    "    data.x = features\n",
    "\n",
    "    fold_results = []\n",
    "\n",
    "    fold_results = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'auc': []\n",
    "        }\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(full_train_df)):\n",
    "        print(f'\\n===== Fold {fold+1}/{k_folds} =====')\n",
    "\n",
    "        # Split train_df for this fold\n",
    "        fold_train_df = full_train_df.iloc[train_idx]\n",
    "        fold_val_df = full_train_df.iloc[val_idx]\n",
    "\n",
    "        # Build datasets\n",
    "        train_dataset = dataset.MLPDataset(\n",
    "            fold_train_df,\n",
    "            node_features_df,\n",
    "            device=device\n",
    "        )\n",
    "        val_dataset = dataset.MLPDataset(\n",
    "            fold_val_df,\n",
    "            node_features_df,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Build dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=dataset.collate_function_mlp)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=dataset.collate_function_mlp)\n",
    "\n",
    "        # Initialize model and optimizer fresh for each fold\n",
    "        input_dim = len(node_features_df['Concat Embedding'][0])\n",
    "        model = models.MLPClassifier(\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=256, \n",
    "            dropout_rate=0.25,\n",
    "            neg_weight=3.0\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "\n",
    "        best_val_auc = 0.0\n",
    "\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            print(f\"\\n----- Epoch {epoch}/{num_epochs} -----\")\n",
    "            train_metrics = train_utils.train_one_epoch_mlp(model, train_loader, optimizer, device)\n",
    "            val_metrics, y_true_val, y_pred_val, y_prob_val = train_utils.evaluate_mlp(model, val_loader, device)\n",
    "\n",
    "            if val_metrics['auc'] > best_val_auc:\n",
    "                best_val_auc = val_metrics['auc']\n",
    "                best_precision = val_metrics['precision']\n",
    "                best_recall = val_metrics['recall']\n",
    "                best_f1 = val_metrics['f1']\n",
    "                train_utils.save_model(model, '../models', experiment=f'k_fold_cross_single_{embedding_to_use}')\n",
    "                print(f\"New best model for fold {fold} saved with AUC: {best_val_auc:.4f}\")\n",
    "\n",
    "        print(f\"\\n=== Fold {fold} Best AUC: {best_val_auc:.4f} ===\")\n",
    "        fold_results['auc'].append(best_val_auc)\n",
    "        fold_results['precision'].append(best_precision)\n",
    "        fold_results['recall'].append(best_recall)\n",
    "        fold_results['f1'].append(best_f1)\n",
    "\n",
    "\n",
    "    print(f\"\\n===== Cross-validation results =====\")\n",
    "    print(f\"\\n===== {embedding_to_use} =====\")\n",
    "    print(f\"per fold: {fold_results}\")\n",
    "    print(f\"Mean AUC: {np.mean(fold_results['auc']):.4f} ± {np.std(fold_results['auc']):.4f}\")\n",
    "    print(f\"Mean Precision: {np.mean(fold_results['precision']):.4f} ± {np.std(fold_results['precision']):.4f}\")\n",
    "    print(f\"Mean Recall: {np.mean(fold_results['recall']):.4f} ± {np.std(fold_results['recall']):.4f}\")\n",
    "    print(f\"Mean F1: {np.mean(fold_results['f1']):.4f} ± {np.std(fold_results['f1']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding_to_use in ['Concatenated Embedding']:#['PINNACLE Embedding', 'ESM Embedding', 'SubCell Embedding', 'Concatenated Embedding']:\n",
    "\n",
    "    node_features_df['Concat Embedding'] = node_features_df[embedding_to_use]\n",
    "\n",
    "    features = torch.stack([\n",
    "        torch.tensor(node_features_df.set_index('Ensembl ID').loc[idx]['Concat Embedding'])\n",
    "        for idx in G.nodes()\n",
    "    ])\n",
    "    data.x = features\n",
    "\n",
    "    fold_results = []\n",
    "\n",
    "    fold_results = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'auc': []\n",
    "        }\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(full_train_df)):\n",
    "        print(f'\\n===== Fold {fold+1}/{k_folds} =====')\n",
    "\n",
    "        # Split train_df for this fold\n",
    "        fold_train_df = full_train_df.iloc[train_idx]\n",
    "        fold_val_df = full_train_df.iloc[val_idx]\n",
    "\n",
    "        # Build datasets\n",
    "        train_dataset = dataset.GNNDataset(\n",
    "            fold_train_df,\n",
    "            active_nodes_df,\n",
    "            node_features_df,\n",
    "            node_to_idx,\n",
    "            device=device\n",
    "        )\n",
    "        val_dataset = dataset.GNNDataset(\n",
    "            fold_val_df,\n",
    "            active_nodes_df,\n",
    "            node_features_df,\n",
    "            node_to_idx,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Build dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=dataset.collate_function_gnn)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=dataset.collate_function_gnn)\n",
    "\n",
    "        # Initialize model and optimizer fresh for each fold\n",
    "        input_dim = len(node_features_df['Concat Embedding'][0])\n",
    "        model = models.GNNClassifier(\n",
    "            input_dim=input_dim, \n",
    "            hidden_dim=512, \n",
    "            output_dim=128, \n",
    "            neg_weight=3.0,\n",
    "            only_active=False\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "\n",
    "        best_val_auc = 0.0\n",
    "\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            print(f\"\\n----- Epoch {epoch}/{num_epochs} -----\")\n",
    "            train_metrics = train_utils.train_one_epoch_gnn(model, data, train_loader, optimizer, device)\n",
    "            val_metrics, y_true_val, y_pred_val, y_prob_val = train_utils.evaluate_gnn(model, data, val_loader, device)\n",
    "\n",
    "            if val_metrics['auc'] > best_val_auc:\n",
    "                best_val_auc = val_metrics['auc']\n",
    "                best_precision = val_metrics['precision']\n",
    "                best_recall = val_metrics['recall']\n",
    "                best_f1 = val_metrics['f1']\n",
    "                train_utils.save_model(model, '../models', experiment=f'k_fold_cross_single_{embedding_to_use}')\n",
    "                print(f\"New best model for fold {fold} saved with AUC: {best_val_auc:.4f}\")\n",
    "\n",
    "        print(f\"\\n=== Fold {fold} Best AUC: {best_val_auc:.4f} ===\")\n",
    "        fold_results['auc'].append(best_val_auc)\n",
    "        fold_results['precision'].append(best_precision)\n",
    "        fold_results['recall'].append(best_recall)\n",
    "        fold_results['f1'].append(best_f1)\n",
    "\n",
    "\n",
    "    print(f\"\\n===== Cross-validation results =====\")\n",
    "    print(f\"\\n===== {embedding_to_use} =====\")\n",
    "    print(f\"per fold: {fold_results}\")\n",
    "    print(f\"Mean AUC: {np.mean(fold_results['auc']):.4f} ± {np.std(fold_results['auc']):.4f}\")\n",
    "    print(f\"Mean Precision: {np.mean(fold_results['precision']):.4f} ± {np.std(fold_results['precision']):.4f}\")\n",
    "    print(f\"Mean Recall: {np.mean(fold_results['recall']):.4f} ± {np.std(fold_results['recall']):.4f}\")\n",
    "    print(f\"Mean F1: {np.mean(fold_results['f1']):.4f} ± {np.std(fold_results['f1']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features_df.drop(['Concat Embedding', 'Concatenated Embedding'], axis=1, inplace=True)\n",
    "node_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_vector(v, max_len):\n",
    "    padded = np.zeros(max_len, dtype=np.float32)\n",
    "    padded[:len(v)] = v\n",
    "    return padded\n",
    "\n",
    "def make_padded_set(row, max_len):\n",
    "    return np.stack([\n",
    "        pad_vector(row['ESM Embedding'], max_len),\n",
    "        pad_vector(row['SubCell Embedding'], max_len),\n",
    "        pad_vector(row['PINNACLE Embedding'], max_len)\n",
    "    ])  # shape: (3, max_len)\n",
    "\n",
    "all_lengths = [\n",
    "    len(vec)\n",
    "    for _, row in node_features_df.iterrows()\n",
    "    for vec in [row['ESM Embedding'], row['SubCell Embedding'], row['PINNACLE Embedding']]\n",
    "]\n",
    "max_len = max(all_lengths)\n",
    "\n",
    "node_features_df['Set Embedding'] = node_features_df.apply(\n",
    "    lambda row: make_padded_set(row, max_len),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "data = from_networkx(G)\n",
    "\n",
    "set_tensor = torch.stack([\n",
    "    torch.tensor(node_features_df.set_index('Ensembl ID').loc[idx]['Set Embedding'])\n",
    "    for idx in G.nodes()\n",
    "])  # shape: (num_nodes, 3, max_len)\n",
    "\n",
    "# Mask where entries are non-zero (i.e., not padded)\n",
    "mask_tensor = (set_tensor != 0).any(dim=-1).to(torch.float32)  # shape: (num_nodes, 3)\n",
    "\n",
    "data.set_features = set_tensor.to(torch.float32)\n",
    "data.set_mask = mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fold_results = []\n",
    "\n",
    "fold_results = {\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': [],\n",
    "    'auc': []\n",
    "    }\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(full_train_df)):\n",
    "    print(f'\\n===== Fold {fold+1}/{k_folds} =====')\n",
    "\n",
    "    # Split train_df for this fold\n",
    "    fold_train_df = full_train_df.iloc[train_idx]\n",
    "    fold_val_df = full_train_df.iloc[val_idx]\n",
    "\n",
    "    # Build datasets\n",
    "    train_dataset = dataset.GNNDataset(\n",
    "        fold_train_df,\n",
    "        active_nodes_df,\n",
    "        node_features_df,\n",
    "        node_to_idx,\n",
    "        device=device\n",
    "    )\n",
    "    val_dataset = dataset.GNNDataset(\n",
    "        fold_val_df,\n",
    "        active_nodes_df,\n",
    "        node_features_df,\n",
    "        node_to_idx,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Build dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=dataset.collate_function_gnn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=dataset.collate_function_gnn)\n",
    "\n",
    "    # Initialize model and optimizer fresh for each fold\n",
    "    input_dim = len(node_features_df['Set Embedding'][0][0])\n",
    "    model = models.GNNAttentionClassifier(\n",
    "        input_dim=input_dim, \n",
    "        pool_hidden_dim=512,\n",
    "        pool_out_dim=256,\n",
    "        gcn_hidden_dim=256,\n",
    "        gcn_out_dim=64, \n",
    "        neg_weight=3.0,\n",
    "        only_active=False, \n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-3, weight_decay=1e-4)\n",
    "\n",
    "    best_val_auc = 0.0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\n----- Epoch {epoch}/{num_epochs} -----\")\n",
    "        train_metrics = train_utils.train_one_epoch_gnn(model, data, train_loader, optimizer, device)\n",
    "        val_metrics, y_true_val, y_pred_val, y_prob_val = train_utils.evaluate_gnn(model, data, val_loader, device)\n",
    "\n",
    "        if val_metrics['auc'] > best_val_auc:\n",
    "            best_val_auc = val_metrics['auc']\n",
    "            best_precision = val_metrics['precision']\n",
    "            best_recall = val_metrics['recall']\n",
    "            best_f1 = val_metrics['f1']\n",
    "            train_utils.save_model(model, '../models', experiment=f'k_fold_cross_single_{embedding_to_use}')\n",
    "            print(f\"New best model for fold {fold} saved with AUC: {best_val_auc:.4f}\")\n",
    "\n",
    "    print(f\"\\n=== Fold {fold} Best AUC: {best_val_auc:.4f} ===\")\n",
    "    fold_results['auc'].append(best_val_auc)\n",
    "    fold_results['precision'].append(best_precision)\n",
    "    fold_results['recall'].append(best_recall)\n",
    "    fold_results['f1'].append(best_f1)\n",
    "\n",
    "\n",
    "print(f\"\\n===== Cross-validation results =====\")\n",
    "print(f\"\\n===== {embedding_to_use} =====\")\n",
    "print(f\"per fold: {fold_results}\")\n",
    "print(f\"Mean AUC: {np.mean(fold_results['auc']):.4f} ± {np.std(fold_results['auc']):.4f}\")\n",
    "print(f\"Mean Precision: {np.mean(fold_results['precision']):.4f} ± {np.std(fold_results['precision']):.4f}\")\n",
    "print(f\"Mean Recall: {np.mean(fold_results['recall']):.4f} ± {np.std(fold_results['recall']):.4f}\")\n",
    "print(f\"Mean F1: {np.mean(fold_results['f1']):.4f} ± {np.std(fold_results['f1']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
